# -*- coding: utf-8 -*-
"""tnu_thwaites_version

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/embedded/projects/bigtimestudios/locations/us-central1/repositories/b014cd62-c05c-4f8f-91f6-9c8c8c4978d5
"""

import pandas as pd
import numpy as np
from google.auth import default
from google.cloud import bigquery
import json

# get big query access
credentials, project = default()
client = bigquery.Client(credentials=credentials, project=project)

##################################
## Get Big Query from the last 24 hours
## ###############################

# Get all design_events from last 24 hours
design_events_live = client.query('''
    select
    event_date,
    event_id,
    user_id,
    event_arrival_ts,
    custom_fields,
    from `bigtimestudios.game_analytics.design_events` a
    where game_id=248246
    and event_id in ('Hardware:MachineSpecs:AllSpecsWeCareAbout')
    and event_arrival_ts > timestamp_sub(current_timestamp(), interval 24 hour)
    ''').to_dataframe()

## Get their current Machine IDs
duplicate_machines = client.query('''
    select distinct
    user_id,
    coalesce(json_extract_scalar(custom_fields, "$.g_analytics.custom.MachineID"),json_extract_scalar(custom_fields, "$.MachineID")) MachineID
    from `bigtimestudios.game_analytics.design_events` a
    join openloot-362008.postgres_rds_auth_api_public.users b on a.user_id=to_hex(sha256(b.id))
    where game_id=248246
    and event_arrival_ts > timestamp_sub(current_timestamp(), interval 24 hour)
    and event_id='Hardware:MachineSpecs:AllSpecsWeCareAbout'
''').to_dataframe()

#######################################
# Get the historical data
#######################################

# get all the user ids
registrations_historical = client.query('''
  select
  id ol_id,
  hashed_id user_id,
  created_at,
  from bigtimestudios.marketplace.auth_registrations
  where created_at > date_sub(current_date(), interval 30 day)
''').to_dataframe()

# Get all the players
users_historical = client.query('''
  select
  distinct user_id
  from bigtimestudios.PlayerAnalytics.player_checkpoint_short
  where event_date > date_sub(current_date(), interval 30 day)
''').to_dataframe()

duplicate_machines_historical = client.query('''
  select
  machine_id MachineID,
  user_id,
  event_arrival_ts,
  from bigtimestudios.true_new_user_datasets.br_users_per_machine_id
''').to_dataframe()

first_purchase_historical = client.query('''
  select
  source_user_id ol_id,
  to_hex(sha256(source_user_id)) user_id,
  min(payment_at) first_purchase
  from bigtimestudios.marketplace.fact_sale_transactions
  group by 1,2
''').to_dataframe()

# Get list of all the users that need a prediction
live_users = set(design_events_live['user_id'])
all_users = set(users_historical['user_id'])

# combine live and historical users to get unique users
all_users = all_users.union(live_users)

# create pandas df of all users
df = pd.DataFrame(all_users, columns=['user_id'])

# Get the MachineID information from design events
def get_machine_id(x):
    if pd.isna(x):
        return None
    try:
        data = json.loads(x)
        machine_id = data.get('g_analytics', {}).get('custom', {}).get('MachineID')
        if machine_id is None:
            machine_id = data.get('MachineID')
        return machine_id
    except (json.JSONDecodeError, AttributeError, TypeError):
        return None

design_events_live['MachineID'] = design_events_live['custom_fields'].apply(get_machine_id)

##############################################################################
## Get machineID information
##############################################################################

# get the data from the last 24
MachineID_last_24 = duplicate_machines[['user_id', 'MachineID']].drop_duplicates()

# get the historical data
MachineID_historical = duplicate_machines_historical[['user_id', 'MachineID']].drop_duplicates()

# vertically append machines the user has been on over last 60
combined_MachineID = pd.concat([MachineID_last_24, MachineID_historical]).drop_duplicates()

##############################################################################
## BR #1: Get the number of machines the user has used over the last 60 days
##############################################################################
unique_machines =  combined_MachineID.groupby('user_id')['MachineID'].nunique().reset_index()
unique_machines = unique_machines.rename(columns={'MachineID': 'unique_machines'})

##############################################################################
## BR #2: Find the number of users that have been on a users' machines over last 60 days
##############################################################################

# get the number of users who have been on that Machine ID in the last 60 days (including last 24 hours)
users_per_machine = combined_MachineID.groupby('MachineID')['user_id'].nunique().reset_index()
users_per_machine = users_per_machine.rename(columns={'user_id':'users_per_machine'})

# join the number of users per machine back to the uniqe user_id:machineID combinations
user_id_machine_users = combined_MachineID.merge(users_per_machine, on='MachineID', how='left')
cousers_per_machine = user_id_machine_users.groupby('user_id')['users_per_machine'].sum().reset_index()

##############################################################################
## BR #2: Find the number of users that have been on a users' machines over last 60 days
##############################################################################

purchase_speed = registrations_historical.merge(first_purchase_historical, on=['ol_id','user_id'], how='left')
#first_purchase_ts['purchase_speed']
purchase_speed['purchase_velocity'] = (purchase_speed['first_purchase'] - purchase_speed['created_at']).dt.total_seconds()/60
purchase_speed.head()

##############################################################################
## Get data with the time to complete FTUE events
##############################################################################

# sql_engine: bigquery
# output_variable: df
# start _sql
_sql = """
with events as (
    SELECT
     event_id,
     ROW_NUMBER() OVER () AS event_number
    FROM UNNEST([
    "Story:Noobland:Adventure:000_RallyPoint:Success",
    "Story:Noobland:Adventure:010_WelcomeDialog:Success",

    -- COMPLETE FTUE 010 - OK
    "Story:Noobland:D01:010_SpeakWithCommBotW_01:Success",
    "Story:Noobland:D01:020_EquipGear:Success",
    "Story:Noobland:Adventure:030_CompleteFTUE010:Success",

    -- BACK TO NOOBLAND - OK
    "Story:Noobland:Adventure:032_ReturnToHub:Success",
    "Story:Noobland:Adventure:033_MysteriousVoice:Success",
    "Story:Noobland:Adventure:035_JumpIntoTeleporter:Success",
    "Story:Noobland:Adventure:040_ExitBuilding:Success",
    "Story:Noobland:Adventure:050_SpeakWithWaffles:Success",

    -- COMPLETE FTUE 020 - OK
    "Story:Noobland:D02:010_RallyPoint:Success",
    "Story:Noobland:D02:020_SpeakWithErwin:Success",
    "Story:Noobland:D02:110_RallyHologram:Success",
    "Story:Noobland:Adventure:060_CompleteFTUE020:Success",

    -- BACK AGAIN TO NOOBLAND - OK
    "Story:Noobland:Adventure:062_GiveHeartToWaffles:Success",

    -- COMPLETE FTUE 030 - OK
    "Story:Noobland:D03:010_RallyPoint:Success",
    "Story:Noobland:D03:090_DestroyGenerators:Success",
    "Story:Noobland:Adventure:080_CompleteFTUE030:Success",
    "Story:Noobland:D03:190_ReturnPortal:Success",
    "Story:WelcomeToEpochCity:Adventure:020_AcquireStarterClassPW:Success"
    ]) AS event_id
),

user_completed_events as (
    select
    user_id,
    event_id,
    min(event_arrival_ts) first_complete
    from `bigtimestudios.game_analytics.design_events`
    where event_date > '2025-10-01'
    and event_id in (
    "Story:Noobland:Adventure:000_RallyPoint:Success",
    "Story:Noobland:Adventure:010_WelcomeDialog:Success",

    # COMPLETE FTUE 010 - OK
    "Story:Noobland:D01:010_SpeakWithCommBotW_01:Success",
    "Story:Noobland:D01:020_EquipGear:Success",
    "Story:Noobland:Adventure:030_CompleteFTUE010:Success",

    # BACK TO NOOBLAND - OK
    "Story:Noobland:Adventure:032_ReturnToHub:Success",
    "Story:Noobland:Adventure:033_MysteriousVoice:Success",
    "Story:Noobland:Adventure:035_JumpIntoTeleporter:Success",
    "Story:Noobland:Adventure:040_ExitBuilding:Success",
    "Story:Noobland:Adventure:050_SpeakWithWaffles:Success",

    # COMPLETE FTUE 020 - OK
    "Story:Noobland:D02:010_RallyPoint:Success",
    "Story:Noobland:D02:020_SpeakWithErwin:Success",
    "Story:Noobland:D02:110_RallyHologram:Success",
    "Story:Noobland:Adventure:060_CompleteFTUE020:Success",

    # BACK AGAIN TO NOOBLAND - OK
    "Story:Noobland:Adventure:062_GiveHeartToWaffles:Success",

    # COMPLETE FTUE 030 - OK
    "Story:Noobland:D03:010_RallyPoint:Success",
    "Story:Noobland:D03:090_DestroyGenerators:Success",
    "Story:Noobland:Adventure:080_CompleteFTUE030:Success",
    "Story:Noobland:D03:190_ReturnPortal:Success",
    "Story:WelcomeToEpochCity:Adventure:020_AcquireStarterClassPW:Success"

    # optional events
    "Story:Noobland:D02:083_UseBuffShrine_Optional:Success",
    "Story:Noobland:D03:015_HologramTable_Optional:Success",
    "Story:Noobland:D03:117_Resonances_Optional:Success",
    "Story:WelcomeToEpochCity:SideQuests:PersonalMetaverse_EconoBot_010_EnterMetaverse:Success",
    "Story:WelcomeToEpochCity:SideQuests:PersonalMetaverse_EconoBot_020_SpeakWithBot:Success"
    )
    group by 1,2
),

user_id_events_spine as (
    select
    distinct a.user_id, b.event_id, b.event_number
    from user_completed_events  a
    cross join events b
)

select
a.user_id,
a.event_number,
a.event_id,
first_complete,
from user_id_events_spine a
left join user_completed_events b on a.user_id = b.user_id and a.event_id = b.event_id
where b.user_id is null
order by user_id, event_number
""" # end _sql
from google.colab.sql import bigquery as _bqsqlcell
df = _bqsqlcell.run(_sql)
df

##############################################################################
## combine business rules
##############################################################################

df_users = purchase_speed.copy()

# Number of machines that the user has used
df_br = df_users.merge(unique_machines, on='user_id', how='left')

# number of cousers on the machines that the user has used
df_br = df_br.merge(cousers_per_machine, on='user_id', how='left')
df_br.sort_values(by='purchase_velocity', ascending=False)